http://www.linuxidc.com/Linux/2015-01/112029.htm
http://www.linuxidc.com/Linux/2015-01/112029p2.htm

1.固定IP
	ifconfig -a
	cat /etc/udev/rules.d/70-persistent-net.rules
	vi /etc/sysconfig/network-scripts/ifcfg-eth2
		/*
			#DEVICE="eth1"
			#BOOTPROTO="dhcp"
			#NM_CONTROLLED="yes"
			#ONBOOT="yes"
			#HWADDR=08:00:27:23:63:BA
			#TYPE="Ethernet"
			#NETMASK=255.255.255.0
			
			
			DEVICE="eth2"
			BOOTPROTO=static
			NM_CONTROLLED="yes"
			ONBOOT="yes"
			HWADDR=08:00:27:91:59:e6
			TYPE="Ethernet"
			IPADDR=10.1.37.39
			NETMASK=255.255.255.0
			GATEWAY=10.1.37.1
		*/

2.创建hadoop用户
	2.1 groupadd hadoop
	2.2 useradd -g hadoop hduser
3.修改hosts文件
	vi /etc/hosts
	添加10.1.37.35 master
		10.1.37.39 slave1
4.ssh无密码登录
	4.2 ssh-keygen -t rsa -P ""
	4.3 cat .ssh/id_rsa.pub >>.ssh/authorized_keys
	 分别将各台机子上的.ssh/id_rsa.pub的内容追加到其他两台的.ssh/authorized_keys中，这样三台机子相互访问就不需要输入密码了。可通过ssh master来测试。
	 scp .ssh/id_rsa.pub hadoop@10.1.37.35:/home/hadoop/id_rsa.pub
	4.4	~/.ssh权限设置为700
		~/.ssh/authorized_keys的权限设置为600
5.hostname
	5.1 hostname master
	5.2 vi /etc/sysconfig/network
	
6.上传hadoop到/home/hadoop
	6.1	tar xzfv hadoop-2.6.2.tar.gz
	6.2 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/hadoop-env.sh 添加jdk (echo $JAVA_HOME查看jdk路径)
		添加export JAVA_HOME=/usr/java/jdk1.8.0_20
	6.3 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/core-site.xml
		添加<configuration>
			    <property>
			        <name>fs.defaultFS</name>
			        <value>hdfs://master:9000</value>
			    </property>
				<property>
				        <name>hadoop.tmp.dir</name>
				        <value>/home/hadoop/tmp</value>
				        <description>Abase for other temporary directories.</description>
				</property>
				<property>
				        <name>io.file.buffer.size</name>
				        <value>4096</value>
				</property>
			</configuration>
	6.4 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/hdfs-site.xml
		添加
			<configuration>
				<property>
	                <name>dfs.namenode.name.dir</name>
	                <value>file:///home/hadoop/dfs/name</value>
		        </property>
		        <property>
	                <name>dfs.datanode.data.dir</name>
	                <value>file:///home/hadoop/dfs/data</value>
		        </property>
		        <property>
	                <name>dfs.replication</name>
	                <value>3</value>
		        </property>
				
			    <property>
			        <name>dfs.namenode.secondary.http-address</name>
			        <value>master:9001</value>
			    </property>
			    <property>
			        <name>dfs.webhdfs.enabled</name>
			        <value>true</value>
			    </property>
			    <property>    
					<name>dfs.permissions</name>    
					<value>false</value>    
				</property>
			</configuration>
	6.5 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/mapred-site.xml
		添加
			<configuration>
		        <property>
		                <name>mapreduce.framework.name</name>
		                <value>yarn</value>
		                <final>true</final>
		        </property>
			    <property>
			        <name>mapreduce.jobhistory.address</name>
			        <value>master:10020</value>
			    </property>
			    <property>
			        <name>mapreduce.jobhistory.webapp.address</name>
			        <value>master:19888</value>
			    </property>
		</configuration>
	6.6 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/yarn-site.xml
		添加
			<configuration>
		        <property>
	                <name>yarn.resourcemanager.hostname</name>
	                <value>master</value>
		        </property>
			
			    <property>
			        <name>yarn.nodemanager.aux-services</name>
			        <value>mapreduce_shuffle</value>
			    </property>
			    <property>
			        <name>yarn.resourcemanager.address</name>
			        <value>master:8032</value>
			    </property>
			    <property>
			        <name>yarn.resourcemanager.scheduler.address</name>
			        <value>master:8030</value>
			    </property>
			    <property>
			        <name>yarn.resourcemanager.resource-tracker.address</name>
			        <value>master:8031</value>
			    </property>
			    <property>
			        <name>yarn.resourcemanager.admin.address</name>
			        <value>master:8033</value>
			    </property>
			    <property>
			        <name>yarn.resourcemanager.webapp.address</name>
			        <value>master:8088</value>
			    </property>
			</configuration>
	6.7 vi /home/hadoop/hadoop-2.6.2/etc/hadoop/slaves
		添加
			slave1
7.复制到slave上
	scp -r  ~/hadoop-2.6.2 hadoop@slave1:~/
8.格式化HDFS文件系统的namenode
	cd hadoop-2.6.2  //进入hadoop-2.6.0目录
	bin/hdfs namenode -format  //格式化
9.启动Hadoop集群
	sbin/start-dfs.sh //开启进程
	sbin/start-yarn.sh
	关闭
		sbin/stop-dfs.sh
		sbin/stop-yarn.sh
10.	http://master:50070/
	yarn:http://10.1.37.100:8088/
11.测试hadoop
	1.1 mkdir input
	1.2 新建file1.txt， file2.txt
	1.3 cat input/f1 Hello world  bye jj
	1.4	hdfs上新建文件夹
		bin/hadoop fs -mkdir /porrylee
		bin/hadoop fs -mkdir /porrylee/input
	1.6 把文件拷贝进hdfs
		bin/hadoop fs  -put ~/input/ /porrylee
	1.7 查看
		bin/hadoop fs -ls /porrylee/input/
	1.8 执行测试程序
		bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.2.jar wordcount /porrylee/input/ /output/wordcount3
	1.9 查看输出
		bin/hdfs dfs -cat /output/wordcount3/*
	1.10删除已经创建的目录
		bin/hadoop fs -rmr /tmp
		bin/hadoop fs -rmr /output
12.eclipse下远程连接hadoop
	2.1安装eclipse-hadoop插件
		hadoop-eclipse-plugin-2.6.0.jar拷贝进eclipse插件目录
	2.2windows下解压hadoop-2.6.0.tar.gz放位置
	2.3填加环境变量
		HADOOP_HOME->D:\software\hadoop-2.6.0
		HADOOP_USER_NAME->root
		root为远程hadoop上用户
		PATH添加->D:\software\hadoop-2.6.0\bin
	2.4解压hadoop_dll2.6.0_64bit.zip并把所有放进D:\software\hadoop-2.6.0\bin
	2.5eclipse中配置Hadoop Map/Reduce
		D:\software\hadoop-2.6.0
	2.6配置Map/Reduce Locations
		Eclipse远程调试Hadoop集群 - Jackson_Mu的个人空间 - 开源中国社区.htm
		并修改Advanced parameters
			hadoop.temp.dir与core-site.xml相同
	2.7带参数运行testMap
		参数
		hdfs://192.168.0.107:9000/porrylee/input/*hdfs中放文件位置*/
		hdfs://192.168.0.107:9000/output2/*输出位置*/
13.重新格式化
	hdfs-ste.xml
	将 dfs.name.dir所指定的目录删除、dfs.data.dir所指定的目录删除
	core-site.xml
	将hadoop.tmp.dir所指定的目录删除。
	重新执行命令：hadoop namenode -format

14.hdfs操作
	14.1文件列表
		hadoop fs -ls //默认路径
		hadoop fs -ls /
	14.2添加目录或文件
		hadoop fs -mkdir /data
		hadoop fs -put /home/hadoop/renho.txt /data
	14.3获取文件
		hadoop fs -get /data/renho.txt /home/hadoop/renho.txt
		hadoop fs -cat /data/renho.txt
	14.4删除
		hadoop fs -rm /data/renho.txt
		hadoop fs -rm -r /data
15.强制离开安全模式
	hadoop dfsadmin -safemode leave
	命令强制离开	
16.修复
	./hdfs fsck /