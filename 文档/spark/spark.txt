1.Scala
	1.1.下载Scala
		http://www.scala-lang.org/download/2.12.0-M3.html
	1.2上传到/usr/local/src
	1.3解压后移动
		tar -zxvf scala-2.12.0-M3.tgz
		mv scala-2.12.0-M3 /usr/lib
	1.4环境变量
		vi /etc/profile
		export SCALA_HOME=/usr/lib/scala-2.12.0-M3
		PATH添加
		export PATH=$PATH:$SCALA_HOME/bin
		source /etc/profile 
2.Spark
	2.1下载spark
		http://spark.apache.org/downloads.html
		spark-1.5.2-bin-hadoop2.6.tgz
	2.2上传到/home/hadoop
	2.3解压
		tar -zvxf spark-1.5.2-bin-hadoop2.6.tgz
	2.4
		vi /etc/pofile
		export HADOOP_HOME=/home/hadoop/hadoop-2.6.2
		#export SPARK_EXAMPLES_JAR=/home/hadoop/spark-1.5.2-bin-without-hadoop/lib/spark-examples-1.5.2-hadoop2.2.0.jar 
		export SPARK_HOME=/home/hadoop/spark-1.5.2-bin-hadoop2.6
		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/bin
		source /etc/profile 
		
		cd conf
		cp spark-env.sh.template spark-env.sh
		cp spark-defaults.conf.template spark-defaults.conf
		vi conf/spark-env.sh
		最后添加
		export JAVA_HOME=/usr/java/jdk1.8.0_20
		export HADOOP_HOME=/home/hadoop/hadoop-2.6.2
 		export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
 		export SPARK_MASTER_IP=10.1.37.100
	2.5测试spark是否成功
		spark-shell --version
		cp slaves.template slaves
	2.6
		bin/spark-shell
		http://192.168.0.107:4040/stages/
3.	spark-shell
	sc
	val file = sc.textFile("hdfs://192.168.0.107:9000/porrylee/input/file1.txt");
	val sparks = file.filter(line => line.contains("hello"));
	sparks.count
	sparks.cache
3.	spark-shell
	sc
	val file = sc.textFile("/root/renho.txt");
	val sparks = file.filter(line => line.contains("hello"));
	sparks.count
	sparks.cache
MASTER=spark://192.168.0.107:7077 ./spark-shell
./start-all.sh
4.master,slave1的hosts添加192.168.0.102 renho

./spark-submit \
--class com.renho.JavaWordCount \
--master spark://192.168.0.107:7077 \
--num-executors 3 \
--driver-memory 1g --executor-memory 1g \
--executor-cores 2 \
/root/a.jar \
hdfs://192.168.0.107:9000/porrylee/input/file1.txt
spark-shell --master spark://192.168.0.107:7077
spark-shell --master local[1]
spark-slave.sh spark://192.168.0.107:7077
java -cp java -cp a.jar:/root/spark-1.5.2-bin-hadoop2.6/lib/spark-assembly-1.5.1-hadoop2.6.0.jar com.renho.JavaWordCount ~/renho.txt:/root/spark-1.5.1-bin-hadoop2.6/lib/spark-assembly-1.5.2-hadoop2.6.0.jar com.renho.JavaWordCount ~/renho.txt
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
spark-submit --master spark://master-renho.hadoop:7077 --class org.apache.spark.examples.SparkPi --name Spark-Pi /root/spark-1.5.1-bin-hadoop2.6/lib/spark-examples-1.5.1-hadoop2.6.0.jar
spark-submit --master spark://master-renho.hadoop:7077 --class com.renho.JavaSparkPi --name JavaSparkPi /root/spark-wordcount-in-java.jar
-Dspark.master=spark://master-renho.hadoop:7077

spark-shell --master U
spark-shell --master spark://master-renho.hadoop:7077

file:///D:/renho.txt

<mirror>
	<id>nexus-osc</id>
	<mirrorOf>*</mirrorOf>
	<name>Nexus osc</name>
	<url>http://maven.oschina.net/content/groups/public/</url>
</mirror>
M2_HOME=D:\software\apache-maven-3.3.3
C:\Users\renho\Downloads\spark-1.5.2\spark-1.5.2>mvn -Dhadoop.version=2.6.2 -Phadoop-2.6 -DskipTests clean package

Spark On Yarn
	export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
	SPARK_JAR=./assembly/target/scala-2.10/spark-assembly_2.10-0.9.1-hadoop2.2.0.jar \
	./bin/spark-class org.apache.spark.deploy.yarn.Client \
	--jar ./examples/target/scala-2.10/spark-examples_2.10-assembly-0.9.1.jar \
	--class org.apache.spark.examples.JavaSparkPi \
	--args yarn-standalone \
	--num-workers 1 \
	--master-memory 512m \
	--worker-memory 512m \
	--worker-cores 1
	
	
	spark-submit --class org.apache.spark.examples.SparkPi \
	    --master yarn-cluster \
	    --num-executors 3 \
	    --driver-memory 1g \
	    --executor-memory 1g \
	    --executor-cores 1 \
	    --queue thequeue \
	    /root/spark-wordcount-in-java.jar \
	    10
	    
	spark-submit --class org.apache.spark.examples.SparkPi \
	    --master yarn-cluster \
	    --num-executors 3 \
	    --driver-memory 1g \
	    --executor-memory 1g \
	    --executor-cores 1 \
	    --queue thequeue \
	    /root/spark-1.5.2-bin-hadoop2.6/lib/spark-examples-1.5.2-hadoop2.6.0.jar \
	    10
	    
	spark-shell --master yarn-client
	
windows下调试
	SPARK_JAVA_OPTS="-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888"
	export JAVA_OPTS="$JAVA_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8888 "
	
	CMD=()
	while IFS= read -d '' -r ARG; do
	  CMD+=("$ARG")
	done < <("$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main $JAVA_OPTS "$@")
	
	if [ "${CMD[0]}" = "usage" ]; then
	  "${CMD[@]}"
	else
	  exec "${CMD[@]}"
	fi