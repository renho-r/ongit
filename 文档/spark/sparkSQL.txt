spark 1.5.2
1.DataFrame Operations
	val sqlContext = new org.apache.spark.sql.SQLContext(sc)
	import sqlContext.implicits._
	val df = sqlContext.read.json("/home/hadoop/user.json")
	df.show()
	df.printSchema()
	df.select("name").show()
	df.select(df("name"), df("age") + 1).show()
	df.filter(df("age") > 21).show()
	df.groupBy("age").count().show()
	df.collect.foreach(row => println(row(0) + ":" + row(1)))
	
2.Inferring the Schema Using Reflection
	val sqlContext = new org.apache.spark.sql.SQLContext(sc)
	import sqlContext.implicits._
	case class User(name: String, age: Int)
	val user = sc.textFile("/home/hadoop/user.txt").map(_.split(",")).map(p => User(p(0), p(1).trim.toInt)).toDF()
	user.registerTempTable("user")
	val teenagers = sqlContext.sql("SELECT name, age FROM user WHERE age >= 13 AND age <= 19")
	teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
	teenagers.map(t => "Name: " + t.getAs[String]("name")).collect().foreach(println)
	teenagers.map(_.getValuesMap[Any](List("name", "age"))).collect().foreach(println)
	
3.Programmatically Specifying the Schema
	val sqlContext = new org.apache.spark.sql.SQLContext(sc)
	val user = sc.textFile("/home/hadoop/user.txt")
	val schemaString = "name age"
	import org.apache.spark.sql.Row;
	import org.apache.spark.sql.types.{StructType,StructField,StringType};
	val schema = StructType(schemaString.split(" ").map(fieldName => StructField(fieldName, StringType, true)))
	val rowRDD = user.map(_.split(",")).map(p => Row(p(0), p(1).trim))
	val userDataFrame = sqlContext.createDataFrame(rowRDD, schema)
	userDataFrame.registerTempTable("user")
	val results = sqlContext.sql("SELECT name FROM user")
	results.map(t => "Name: " + t(0)).collect().foreach(println)
	
4.Generic Load/Save Functions
	val df = sqlContext.read.load("/home/hadoop/users.parquet")
	df.select("name", "favorite_color").write.save("namesAndFavColors.parquet")
5.Manually Specifying Options
	val df = sqlContext.read.format("json").load("/home/hadoop/user.json")
	df.select("name", "age").write.format("parquet").save("namesAndAges.parquet")	
	
6.Schema Merging
	import sqlContext.implicits._
	val df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF("single", "double")
	df1.write.parquet("/home/hadoop/spark/sql/data/test_table/key=1")
	val df2 = sc.makeRDD(6 to 10).map(i => (i, i * 3)).toDF("single", "triple")
	df2.write.parquet("/home/hadoop/spark/sql/data/test_table/key=2")
	val df3 = sqlContext.read.option("mergeSchema", "true").parquet("/home/hadoop/spark/sql/data/test_table")
	df3.printSchema()

	
/*7.Hive
	val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
	sqlContext.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
	sqlContext.sql("LOAD DATA LOCAL INPATH '/home/hadoop/kv1.txt' INTO TABLE src")
	sqlContext.sql("FROM src SELECT key, value").collect().foreach(println)
*/
	